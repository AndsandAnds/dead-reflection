name: reflections

services:
  db:
    image: pgvector/pgvector:pg18
    # Optional perf knob (Linux-only): try io_method=io_uring. We default to
    # "worker" and only change this after benchmarking.
    command: ["postgres", "-c", "io_method=${POSTGRES_IO_METHOD:-worker}"]
    environment:
      POSTGRES_DB: ${POSTGRES_DB:-reflections}
      POSTGRES_USER: ${POSTGRES_USER:-reflections}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:?Set POSTGRES_PASSWORD in your .env}
    ports:
      - "127.0.0.1:${POSTGRES_PORT:-5432}:5432"
    volumes:
      # Postgres 18+ images expect the mount at /var/lib/postgresql so data is
      # stored in a major-version-specific subdirectory (simplifies upgrades).
      - db_data:/var/lib/postgresql
      # allow pgbench scripts to be referenced from inside the container
      - ./bench:/workspace/bench:ro
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-reflections} -d ${POSTGRES_DB:-reflections}"]
      interval: 3s
      timeout: 3s
      retries: 20

  api:
    build:
      context: .
      dockerfile: docker/api/Dockerfile
      target: dev
    environment:
      REFLECTIONS_DB_HOST: db
      REFLECTIONS_DB_PORT: "5432"
      REFLECTIONS_DB_NAME: ${POSTGRES_DB:-reflections}
      REFLECTIONS_DB_USER: ${POSTGRES_USER:-reflections}
      REFLECTIONS_DB_PASSWORD: ${POSTGRES_PASSWORD:?Set POSTGRES_PASSWORD in your .env}
      # Ollama is expected to be installed on the macOS host for Metal acceleration.
      # On Docker Desktop, "host.docker.internal" resolves to the host.
      OLLAMA_BASE_URL: ${OLLAMA_BASE_URL:-http://host.docker.internal:11434}
      OLLAMA_MODEL: ${OLLAMA_MODEL:-llama3.2:latest}
      OLLAMA_TIMEOUT_S: ${OLLAMA_TIMEOUT_S:-30}
      # Host-run speech services (macOS): whisper.cpp and say/afconvert bridges.
      STT_BASE_URL: ${STT_BASE_URL:-}
      STT_TIMEOUT_S: ${STT_TIMEOUT_S:-120}
      TTS_BASE_URL: ${TTS_BASE_URL:-}
      TTS_TIMEOUT_S: ${TTS_TIMEOUT_S:-30}
      MEMORY_AUTO_INGEST: ${MEMORY_AUTO_INGEST:-true}
      MEMORY_CHUNK_TURN_WINDOW: ${MEMORY_CHUNK_TURN_WINDOW:-2}
      # Avatar image generation (Diffusers SDXL)
      AVATAR_IMAGE_ENGINE: ${AVATAR_IMAGE_ENGINE:-diffusers_sdxl}
      DIFFUSERS_SDXL_BASE_MODEL: ${DIFFUSERS_SDXL_BASE_MODEL:-/app/stable-diffusion/base}
      DIFFUSERS_SDXL_REFINER_MODEL: ${DIFFUSERS_SDXL_REFINER_MODEL:-/app/stable-diffusion/refiner}
      DIFFUSERS_LOCAL_FILES_ONLY: ${DIFFUSERS_LOCAL_FILES_ONLY:-true}
      # NOTE: Docker on macOS cannot use Metal/MPS. Default to CPU for containers.
      # For MPS acceleration, run a host bridge and point the API at it.
      DIFFUSERS_DEVICE: ${DIFFUSERS_DEVICE:-cpu}
      DIFFUSERS_DTYPE: ${DIFFUSERS_DTYPE:-float32}
      DIFFUSERS_HIGH_NOISE_FRAC: ${DIFFUSERS_HIGH_NOISE_FRAC:-0.8}
      DIFFUSERS_ENABLE_COMPILE: ${DIFFUSERS_ENABLE_COMPILE:-false}
    depends_on:
      db:
        condition: service_healthy
    ports:
      - "127.0.0.1:${API_PORT:-8000}:8000"
    volumes:
      - ./:/app
    command: ["poetry", "run", "uvicorn", "reflections.api.main:app", "--host", "0.0.0.0", "--port", "8000", "--reload"]

  ui:
    build:
      context: .
      dockerfile: docker/ui/Dockerfile
      target: dev
    environment:
      NEXT_PUBLIC_API_BASE_URL: ${NEXT_PUBLIC_API_BASE_URL:-http://localhost:${API_PORT:-8000}}
    depends_on:
      - api
    ports:
      - "127.0.0.1:${UI_PORT:-3000}:3000"
    volumes:
      - ./apps/web:/app
      - ui_node_modules:/app/node_modules
    command: ["npm", "run", "dev", "--", "--hostname", "0.0.0.0", "--port", "3000"]

volumes:
  db_data:
  ui_node_modules:


